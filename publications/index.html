<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Anthony Platanios | publications</title>
  <meta name="description" content="Personal website of Anthony Platanios.">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/publications/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Anthony</span> Platanios</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    cv
                    
                  </a>
              </li>
            
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    projects
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item navbar-active font-weight-bold">
                  <a class="nav-link" href="/publications/">
                    publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/teaching/">
                    teaching
                    
                  </a>
              </li>
            
          
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>publications</h1>
  <h6><span>*</span> denotes equal contribution and joint lead authorship.</h6>



<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://aclanthology.org/venues/emnlp/" target="_blank">
          EMNLP
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="StengelEskin-2022-troubling_quirk" class="col p-0">
      <h5 class="title mb-0">When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="https://esteng.github.io/" target="_blank">Elias Stengel-Eskin</a>,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/adam-pauls-137054102" target="_blank">Adam Pauls</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://samthomson.com/" target="_blank">Sam Thomson</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://hao-fang.github.io/" target="_blank">Hao Fang</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.cs.jhu.edu/~vandurme/" target="_blank">Benjamin Van Durme</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.cs.jhu.edu/~jason/" target="_blank">Jason Eisner</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="https://ysu1989.github.io/" target="_blank">Yu Su</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Conference on Empirical Methods in Natural Language Processing (EMNLP)
          
          
            2022.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#StengelEskin-2022-troubling_quirk-abstract" role="button" aria-expanded="false" aria-controls="StengelEskin-2022-troubling_quirk-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/stengel_eskin_2022_troubling_quirk/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/microsoft/nlu-incremental-symbol-learning" target="_blank">Code</a>
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="StengelEskin-2022-troubling_quirk-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            In natural language understanding (NLU) production systems, users’ evolving needs necessitate the addition of new features over time, indexed by new symbols added to the meaning representation space. This requires additional training data and results in ever-growing datasets. We present the first systematic investigation into this incremental symbol learning scenario. Our analyses reveal a troubling quirk in building (broad-coverage) NLU systems: as the training dataset grows, more data is needed to learn new symbols, forming a vicious cycle. We show that this trend holds for multiple mainstream models on two common NLU tasks: intent recognition and semantic parsing. Rejecting class imbalance as the sole culprit, we reveal that the trend is closely associated with an effect we call source signal dilution, where strong lexical cues for the new symbol become diluted as the training dataset grows. Selectively dropping training examples to prevent dilution often reverses the trend, showing the over-reliance of mainstream neural NLU models on simple lexical cues and their lack of contextual understanding.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://aclanthology.org/venues/acl/" target="_blank">
          ACL
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Zhou-2022-online_parsing" class="col p-0">
      <h5 class="title mb-0">Online Semantic Parsing for Latency Reduction in Task-Oriented Dialogue.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="https://github.com/jzhou316" target="_blank">Jiawei Zhou</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.cs.jhu.edu/~jason/" target="_blank">Jason Eisner</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/mike-newman-82a49b5" target="_blank">Michael Newman</a>,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="https://samthomson.com/" target="_blank">Sam Thomson</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Annual Meeting of the Association for Computational Linguistics (ACL)
          
          
            2022.
          
        </p>
      </div>

      
      <div style="color: #FFC000;"><p class="periodical font-italic"><i class="fa fa-solid fa-star"></i> Outstanding Paper Award!</p></div>
      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Zhou-2022-online_parsing-abstract" role="button" aria-expanded="false" aria-controls="Zhou-2022-online_parsing-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/zhou_2022_online_parsing/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/microsoft/online-semantic-parsing-for-latency-reduction" target="_blank">Code</a>
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Zhou-2022-online_parsing-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Standard conversational semantic parsing maps a complete user utterance into an executable program, after which the program is executed to respond to the user. This could be slow when the program contains expensive function calls. We investigate the opportunity to reduce latency by predicting and executing function calls while the user is still speaking. We introduce the task of online semantic parsing for this purpose, with a formal latency reduction metric inspired by simultaneous machine translation. We propose a general framework with first a learned prefix-to-program prediction module, and then a simple yet effective thresholding heuristic for subprogram selection for early execution. Experiments on the SMCalFlow and TreeDST datasets show our approach achieves large latency reduction with good parsing quality, with a 30%–65% latency reduction depending on function execution time and allowed cost.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://aclanthology.org/venues/acl/" target="_blank">
          ACL
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Chen-2022-text_to_sql" class="col p-0">
      <h5 class="title mb-0">Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="http://www.chenz.umiacs.io/" target="_blank">Chen Zhao</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://ysu1989.github.io/" target="_blank">Yu Su</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/adam-pauls-137054102" target="_blank">Adam Pauls</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                <nobr><em>Emmanouil Antonios Platanios</em>.</nobr>
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Annual Meeting of the Association for Computational Linguistics (ACL)
          
          
            2022.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Chen-2022-text_to_sql-abstract" role="button" aria-expanded="false" aria-controls="Chen-2022-text_to_sql-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/chen_2022_text_to_sql/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/microsoft/text-to-sql-schema-expansion-generalization" target="_blank">Code</a>
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Chen-2022-text_to_sql-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Text-to-SQL parsers map natural language questions to programs that are executable over tables to generate answers, and are typically evaluated on large-scale datasets like Spider (Yu et al., 2018). We argue that existing benchmarks fail to capture a certain out-of-domain generalization problem that is of significant practical importance: matching domain specific phrases to composite operation over columns. To study this problem, we first propose a synthetic dataset along with a re-purposed train/test split of the Squall dataset (Shi et al., 2020) as new benchmarks to quantify domain generalization over column operations, and find existing state-of-the-art parsers struggle in these benchmarks. We propose to address this problem by incorporating prior domain knowledge by preprocessing table schemas, and design a method that consists of two components: schema expansion and schema pruning. This method can be easily applied to multiple existing base parsers, and we show that it significantly outperforms baseline parsers on this domain generalization problem, boosting the underlying parsers’ overall performance by up to 13.8% relative accuracy gain (5.1% absolute) on the new Squall data split.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://aclanthology.org/venues/acl/" target="_blank">
          ACL
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Belyy-2022-k_best" class="col p-0">
      <h5 class="title mb-0">Guided K-best Selection for Semantic Parsing Annotation.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="https://avbelyy.github.io/" target="_blank">Anton Belyy*</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://appleternity.github.io/chieh-yang/" target="_blank">Chieh-Yang Huang*</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.mit.edu/~jda/" target="_blank">Jacob Andreas</a>,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://samthomson.com/" target="_blank">Sam Thomson</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://rshin.github.io/" target="_blank">Richard Shin</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://sroy9.github.io/" target="_blank">Subhro Roy</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/alex-nisnevich-514a5836" target="_blank">Aleksandr Nisnevich</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/neocatalyst" target="_blank">Charles Chen</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="https://www.cs.jhu.edu/~vandurme/" target="_blank">Benjamin Van Durme</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Annual Meeting of the Association for Computational Linguistics (ACL)
          
          
            2022.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Belyy-2022-k_best-abstract" role="button" aria-expanded="false" aria-controls="Belyy-2022-k_best-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/belyy_2022_k_best/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Belyy-2022-k_best-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Collecting data for conversational semantic parsing is a time-consuming and demanding process. In this paper we consider, given an incomplete dataset with only a small amount of data, how to build an AI-powered human-in-the-loop process to enable efficient data collection. A guided K-best selection process is proposed, which (i) generates a set of possible valid candidates; (ii) allows users to quickly traverse the set and filter incorrect parses; and (iii) asks users to select the correct parse, with minimal modification when necessary. We investigate how to best support users in efficiently traversing the candidate set and locating the correct parse, in terms of speed and accuracy. In our user study, consisting of five annotators labeling 300 instances each, we find that combining keyword searching, where keywords can be used to query relevant candidates, and keyword suggestion, where representative keywords are automatically generated, enables fast and accurate annotation.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2022</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.aaai.org" target="_blank">
          AAAI
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Stoica-2021-retacred" class="col p-0">
      <h5 class="title mb-0">Re-TACRED: Addressing Shortcomings of the TACRED Dataset.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/george--stoica/" target="_blank">George Stoica</a>,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~bapoczos/" target="_blank">Barnabás Póczos</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In AAAI Conference on Artificial Intelligence
          
          
            2021.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Stoica-2021-retacred-abstract" role="button" aria-expanded="false" aria-controls="Stoica-2021-retacred-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/stoica_2021_retacred/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Stoica-2021-retacred-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            TACRED is one of the largest and most widely used sentence-level relation extraction datasets. Proposed models that are evaluated using this dataset consistently set new state-of-the-art performance. However, they still exhibit large error rates despite leveraging external knowledge and unsupervised pretraining on large text corpora. A recent study suggested that this may be due to poor dataset quality. The study observed that over 50% of the most challenging sentences from the development and test sets are incorrectly labeled and account for an average drop of 8% f1-score in model performance. However, this study was limited to a small biased sample of 5k (out of a total of 106k) sentences, substantially restricting the generalizability and broader implications of its findings. In this paper, we address these shortcomings by: (i) performing a comprehensive study over the whole TACRED dataset, (ii) proposing an improved crowdsourcing strategy and deploying it to re-annotate the whole dataset, and (iii) performing a thorough analysis to understand how correcting the TACRED annotations affects previously published results. After verification, we observed that 23.9% of TACRED labels are incorrect. Moreover, evaluating several models on our revised dataset yields an average f1-score improvement of 14.3% and helps uncover significant relationships between the different models (rather than simply offsetting or scaling their scores by a constant factor). Finally, aside from our analysis we also release Re-TACRED, a new completely re-annotated version of the TACRED dataset that can be used to perform reliable evaluation of relation extraction models.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://aclanthology.org/venues/emnlp/" target="_blank">
          EMNLP
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Shin-2021-clamp" class="col p-0">
      <h5 class="title mb-0">Constrained Language Models Yield Few-Shot Semantic Parsers.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="https://rshin.github.io/" target="_blank">Richard Shin</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://christopherhlin.com/" target="_blank">Christopher Lin</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://samthomson.com/" target="_blank">Sam Thomson</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/neocatalyst" target="_blank">Charles Chen</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://sroy9.github.io/" target="_blank">Subhro Roy</a>,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/adam-pauls-137054102" target="_blank">Adam Pauls</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/klein.html" target="_blank">Dan Klein</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.cs.jhu.edu/~jason/" target="_blank">Jason Eisner</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="https://www.cs.jhu.edu/~vandurme/" target="_blank">Benjamin Van Durme</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Conference on Empirical Methods in Natural Language Processing (EMNLP)
          
          
            2021.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Shin-2021-clamp-abstract" role="button" aria-expanded="false" aria-controls="Shin-2021-clamp-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/shin_2021_clamp/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Shin-2021-clamp-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://aclanthology.org/venues/acl/" target="_blank">
          ACL
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2021-platypus" class="col p-0">
      <h5 class="title mb-0">Value-Agnostic Conversational Semantic Parsing.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/adam-pauls-137054102" target="_blank">Adam Pauls</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://sroy9.github.io/" target="_blank">Subhro Roy</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://zhangyuc.github.io/" target="_blank">Yuchen Zhang</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>Alex Kyte,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://people.csail.mit.edu/aguo/" target="_blank">Alan Guo</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://samthomson.com/" target="_blank">Sam Thomson</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://allenai.org/team/jayantk/" target="_blank">Jayant Krishnamurthy</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/w01fe/" target="_blank">Jason Wolfe</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.mit.edu/~jda/" target="_blank">Jacob Andreas</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/klein.html" target="_blank">Dan Klein</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Annual Meeting of the Association for Computational Linguistics (ACL)
          
          
            2021.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2021-platypus-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2021-platypus-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2021_platypus/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/../pptx/platanios_2021_platypus/slides.pptx" target="_blank">Slides</a>
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2021-platypus-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a model that abstracts over values to focus prediction on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency. Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. Trained on the SMCalFlow and TreeDST datasets, our model outperforms prior work by 7.3% and 10.6% respectively in terms of absolute accuracy. Trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4% and 6.4%. These results indicate that simple representations are key to effective generalization in conversational semantic parsing.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://iclr.cc/" target="_blank">
          ICLR
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Xian-2021-hyper_dynamics" class="col p-0">
      <h5 class="title mb-0">HyperDynamics: Generating Expert Dynamics Models by Observation.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr>Zhou Xian,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://shamitlal.github.io/" target="_blank">Shamit Lal</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://sfish0101.bitbucket.io/" target="_blank">Hsiao-Yu Fish Tung</a>,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="https://www.cs.cmu.edu/~katef/" target="_blank">Katerina Fragkiadaki</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In International Conference on Learning Representations
          
          
            2021.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Xian-2021-hyper_dynamics-abstract" role="button" aria-expanded="false" aria-controls="Xian-2021-hyper_dynamics-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/xian_2021_hyper_dynamics/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Xian-2021-hyper_dynamics-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We propose HD, a dynamics meta-learning framework that conditions on an agent’s interactions with the environment and optionally its visual observations, and generates the parameters of neural dynamics models based on inferred properties of the dynamical system. Physical and visual properties of the environment that are not part of the low-dimensional state yet affect its temporal dynamics are inferred from the interaction history and visual observations, and are implicitly captured in the generated parameters. We test HyperDynamics on a set of object pushing and locomotion tasks. It outperforms existing dynamics models in the literature that adapt to environment variations by learning dynamics over high dimensional visual observations, capturing the interactions of the agent in recurrent state representations, or using gradient-based meta-optimization. We also show our method matches the performance of an ensemble of separately trained experts, while also being able to generalize well to unseen environment variations at test time. We attribute its good performance to the multiplicative interactions between the inferred system properties—captured in the generated parameters—and the low-dimensional state representation of the dynamical system.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://aclanthology.org/venues/naacl/" target="_blank">
          NAACL
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Yin-2021-compositional_generalization" class="col p-0">
      <h5 class="title mb-0">Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="https://pcyin.me/" target="_blank">Pengcheng Yin</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://hao-fang.github.io/" target="_blank">Hao Fang</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.phontron.com/" target="_blank">Graham Neubig</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/adam-pauls-137054102" target="_blank">Adam Pauls</a>,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://ysu1989.github.io/" target="_blank">Yu Su</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://samthomson.com/" target="_blank">Sam Thomson</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="https://www.mit.edu/~jda/" target="_blank">Jacob Andreas</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)
          
          
            2021.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Yin-2021-compositional_generalization-abstract" role="button" aria-expanded="false" aria-controls="Yin-2021-compositional_generalization-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/yin_2021_compositional_generalization/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/microsoft/compositional-generalization-span-level-attention" target="_blank">Code</a>
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Yin-2021-compositional_generalization-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2021</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.ml.cmu.edu" target="_blank">
          CMU
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2020-thesis" class="col p-0">
      <h5 class="title mb-0">Learning Collections of Functions.</h5>
      <div class="author">
        
          
          
            
              <nobr><em>Emmanouil Antonios Platanios</em>.</nobr>
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Doctoral Thesis at Carnegie Mellon University
          
          
            2020.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2020-thesis-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2020-thesis-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/thesis/thesis.pdf" target="_blank">PDF</a>
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/thesis/defense.pdf" target="_blank">Slides</a>
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2020-thesis-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Human intelligence is magnificent. One of its most impressive aspects is how humans always seem able to learn new skills quickly and without much supervision by utilizing previously learned skills and forming connections between them. More specifically, human learning is often not about learning a single skill in isolation, but rather about learning collections of skills and utilizing relationships between them to learn more efficiently. Furthermore, these relationships may either be explicitly provided or implicitly learned, indicating high levels of abstraction in the learned abilities. On the other hand, even though machine learning has witnessed growing success across a multitude of applications over the past years, current systems are each highly specialized to solve one or just a handful of problems. In this thesis, we argue that a computer system that learns to perform multiple tasks jointly and that is aware of the relationships between these tasks, will be able to learn more efficiently and effectively than a system that learns to perform each task in isolation. Moreover, the relationships between the tasks may either be explicitly provided through supervision or implicitly learned by the system itself, and will allow the system to self-reflect and evaluate itself without any task-specific supervision. This includes learning relationships in the form of higher-order functions—namely functions that compose, transform, or otherwise manipulate other functions—that can enable truly multi-task and zero-shot learning. In the first part, we present a method that allows learning systems to evaluate themselves in an unsupervised manner by leveraging explicitly provided relationships between multiple learned functions. We refer to this ability as self-reflection and show how it addresses an important limitation of existing never-ending learning systems like the never-ending language learner (Mitchell et al., 2018). We then propose multiple extensions that improve upon this method, resulting in several robust algorithms for estimating the accuracy of classifiers from unlabeled data. In the second part, we consider more general multi-task learning settings and propose an abstract framework called contextual parameter generation (CPG), which allows systems to generate functions for solving different kinds of tasks without necessarily having been shown any training data for these tasks. This framework generalizes existing approaches in multi-task learning, transfer learning, and meta-learning, and it further allows for learning arbitrary higher-order functions. It does so by formalizing the notion of a function representation and what it means for functions to operate on other functions or even on themselves. This new type of learning, which we refer to as higher-order learning, enables learning relationships between multiple functions in the form of higher-order functions, and is inspired by functional programming and category theory. Finally, we propose the jelly bean world (JBW), a novel evaluation framework for never-ending learning systems.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://iclr.cc/" target="_blank">
          ICLR
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2020-jbw" class="col p-0">
      <h5 class="title mb-0">Jelly Bean World: A Testbed for Never-Ending Learning.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios*</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://asaparov.org/" target="_blank">Abulhair Saparov*</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~tom/" target="_blank">Tom Mitchell</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In International Conference on Learning Representations
          
          
            2020.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2020-jbw-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2020-jbw-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios-2020-jbw-paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/eaplatanios/jelly-bean-world" target="_blank">Code</a>
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2020-jbw-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Machine learning has shown growing success in recent years. However, current machine learning systems are highly specialized, trained for particular problems or domains, and typically on a single narrow dataset. Human learning, on the other hand, is highly general and adaptable. Never-ending learning is a machine learning paradigm that aims to bridge this gap, with the goal of encouraging researchers to design machine learning systems that can learn to perform a wider variety of inter-related tasks in more complex environments. To date, there is no environment or testbed to facilitate the development and evaluation of never-ending learning systems. To this end, we propose the Jelly Bean World testbed. The Jelly Bean World allows experimentation over two-dimensional grid worlds which are filled with items and in which agents can navigate. This testbed provides environments that are sufficiently complex and where more generally intelligent algorithms ought to perform better than current state-of-the-art reinforcement learning approaches. It does so by producing non-stationary environments and facilitating experimentation with multi-task, multi-agent, multi-modal, and curriculum learning settings. We hope that this new freely-available software will prompt new research and interest in the development and evaluation of never-ending learning systems and more broadly, general intelligence systems.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.aaai.org" target="_blank">
          AAAI
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2020-coper" class="col p-0">
      <h5 class="title mb-0">Contextual Parameter Generation for Knowledge Graph Link Prediction.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios*</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.linkedin.com/in/george--stoica/" target="_blank">George Stoica*</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://otiliastr.github.io/" target="_blank">Otilia Stretcu*</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~bapoczos/" target="_blank">Barnabás Póczos</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~tom/" target="_blank">Tom Mitchell</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In AAAI Conference on Artificial Intelligence
          
          
            2020.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2020-coper-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2020-coper-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2020_coper/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/otiliastr/coper" target="_blank">Code</a>
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2020-coper-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We consider the task of knowledge graph link prediction. Given a question consisting of a source entity and a relation (e.g., Shakespeare and BornIn), the objective is to predict the most likely answer entity (e.g., England). Recent approaches tackle this problem by learning entity and relation embeddings. However, they often constrain the relationship between these embeddings to be additive (i.e., the embeddings are concatenated and then processed by a sequence of linear functions and element-wise non-linearities). We show that this type of interaction significantly limits representational power. For example, such models cannot handle cases where a different projection of the source entity is used for each relation. We propose to use contextual parameter generation to address this limitation. More specifically, we treat relations as the context in which source entities are processed to produce predictions, by using relation embeddings to generate the parameters of a model operating over source entity embeddings. This allows models to represent more complex interactions between entities and relations. We apply our method on two existing link prediction methods, including the current state-of-the-art, resulting in significant performance gains and establishing a new state-of-the-art for this task. These gains are achieved while also reducing training time by up to 28 times.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2020</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.nips.cc" target="_blank">
          NeurIPS
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Stretcu-2019" class="col p-0">
      <h5 class="title mb-0">Graph Agreement Models for Semi-Supervised Learning.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="https://otiliastr.github.io/" target="_blank">Otilia Stretcu</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>Krishnamurthy Viswanathan,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>Dana Movshovitz-Attias,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr>Sujith Ravi,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr>Andrew Tomkins</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Neural Information Processing Systems
          
          
            2019.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Stretcu-2019-abstract" role="button" aria-expanded="false" aria-controls="Stretcu-2019-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/stretcu_2019_gam/paper.pdf" target="_blank">PDF</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/stretcu_2019_gam/supplementary.pdf" target="_blank">Supplementary</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/stretcu_2019_gam/poster.pdf" target="_blank">Poster</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/tensorflow/neural-structured-learning" target="_blank">Code</a>
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Stretcu-2019-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Graph-based algorithms are among the most successful paradigms for solving semi-supervised learning tasks. Recent work on graph convolutional networks and neural graph learning methods has successfully combined the expressiveness of neural networks with graph structures. We propose a technique that, when applied to these methods, achieves state-of-the-art results on semi-supervised learning datasets. Traditional graph-based algorithms, such as label propagation, were designed with the underlying assumption that the label of a node can be imputed from that of the neighboring nodes. However, real-world graphs are either noisy or have edges that do not correspond to label agreement. To address this, we propose Graph Agreement Models (GAM), which introduces an auxiliary model that predicts the probability of two nodes sharing the same label as a learned function of their features. The agreement model is used when training a node classification model by encouraging agreement only for the pairs of nodes it deems likely to have the same label, thus guiding its parameters to better local optima. The classification and agreement models are trained jointly in a co-training fashion. Moreover, GAM can also be applied to any semi-supervised classification problem, by inducing a graph whenever one is not provided. We demonstrate that our method achieves a relative improvement of up to 72% for various node classification models, and obtains state-of-the-art results on multiple established datasets.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://aclanthology.org/venues/naacl/" target="_blank">
          NAACL
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2019" class="col p-0">
      <h5 class="title mb-0">Competence-based Curriculum Learning for Neural Machine Translation.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://otiliastr.github.io/" target="_blank">Otilia Stretcu</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.phontron.com/" target="_blank">Graham Neubig</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~bapoczos/" target="_blank">Barnabás Póczos</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~tom/" target="_blank">Tom Mitchell</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)
          
          
            2019.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2019-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2019-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2019_curriculum_nmt/paper.pdf" target="_blank">PDF</a>
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2019_curriculum_nmt/poster.pdf" target="_blank">Poster</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2019_curriculum_nmt/slides.pdf" target="_blank">Slides</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/eaplatanios/symphony-mt" target="_blank">Code</a>
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2019-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2019</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://aclanthology.org/venues/emnlp/" target="_blank">
          EMNLP
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2018" class="col p-0">
      <h5 class="title mb-0">Contextual Parameter Generation for Universal Neural Machine Translation.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.mrinmaya.io/" target="_blank">Mrinmaya Sachan</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.phontron.com/" target="_blank">Graham Neubig</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~tom/" target="_blank">Tom Mitchell</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Conference on Empirical Methods in Natural Language Processing (EMNLP)
          
          
            2018.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2018-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2018-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2018_cpg_nmt/paper.pdf" target="_blank">PDF</a>
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2018_cpg_nmt/poster.pdf" target="_blank">Poster</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2018_cpg_nmt/slides.pdf" target="_blank">Slides</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/eaplatanios/symphony-mt" target="_blank">Code</a>
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2018-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We propose a simple modification to existing neural machine translation (NMT) models that enables using a single universal model to translate between multiple languages while allowing for language specific parameterization, and that can also be used for domain adaptation. Our approach requires no changes to the model architecture of a standard NMT system, but instead introduces a new component, the contextual parameter generator (CPG), that generates the parameters of the system (e.g., weights in a neural network). This parameter generator accepts source and target language embeddings as input, and generates the parameters for the encoder and the decoder, respectively. The rest of the model remains unchanged and is shared across all languages. We show how this simple modification enables the system to use monolingual data for training and also perform zero-shot translation. We further show it is able to surpass state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and that the learned language embeddings are able to uncover interesting relationships between languages.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://cacm.acm.org" target="_blank">
          CACM
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Mitchell-2018-cacm" class="col p-0">
      <h5 class="title mb-0">Never-Ending Learning.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~tom/" target="_blank">Tom Mitchell</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>William W Cohen,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>Estevam R Hruschka Jr,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://talukdar.net/" target="_blank">Partha Pratim Talukdar</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~bishan/" target="_blank">Bishan Yang</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~jbetter/" target="_blank">Justin Betteridge</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~acarlson/" target="_blank">Andrew Carlson</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://allenai.org/team/bhavanad/" target="_blank">Bhanava Dalvi</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~mg1/" target="_blank">Matt Gardner</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>Bryan Kisiel,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://allenai.org/team/jayantk/" target="_blank">Jayant Krishnamurthy</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~nlao/" target="_blank">Ni Lao</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~krivard/" target="_blank">Kathryn Mazaitis</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>Thahir P Mohamed,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://nakashole.com/" target="_blank">Ndapakula Nakashole</a>,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://aritter.github.io/" target="_blank">Alan Ritter</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.mehdisamadi.com/" target="_blank">Mehdi Samadi</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://burrsettles.com/" target="_blank">Burr Settles</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~rcwang/" target="_blank">Richard C Wang</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~dwijaya/" target="_blank">Derry Wijaya</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~abhinavg/" target="_blank">Abhinav Gupta</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~xinleic/" target="_blank">Xinlei Chen</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://asaparov.org/" target="_blank">Abulhair Saparov</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.malcolmgreaves.io/" target="_blank">Malcolm Greaves</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://staff.psc.edu/welling/" target="_blank">Joel Welling</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Communications of the ACM
          
          
            2018.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Mitchell-2018-cacm-abstract" role="button" aria-expanded="false" aria-controls="Mitchell-2018-cacm-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/mitchell_2018_cacm/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Mitchell-2018-cacm-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Whereas people learn many different types of knowledge from diverse experiences over many years, and become better learners over time, most current machine learning systems are much more narrow, learning just a single function or data model based on statistical analysis of a single data set. We suggest that people learn better than computers precisely because of this difference, and we suggest a key direction for machine learning research is to develop software architectures that enable intelligent agents to also learn many types of knowledge, continuously over many years, and to become better learners over time. In this paper we define more precisely this never-ending learning paradigm for machine learning, and we present one case study: the Never-Ending Language Learner (NELL), which achieves a number of the desired properties of a never-ending learner. NELL has been learning to read the Web 24hrs/day since January 2010, and so far has acquired a knowledge base with 120mn diverse, confidence-weighted beliefs (e.g., servedWith(tea,biscuits)), while learning thousands of interrelated functions that continually improve its reading competence over time. NELL has also learned to reason over its knowledge base to infer new beliefs it has not yet read from those it has, and NELL is inventing new relational predicates to extend the ontology it uses to represent beliefs. We describe the design of NELL, experimental results illustrating its behavior, and discuss both its successes and shortcomings as a case study in never-ending learning. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://arxiv.org" target="_blank">
          arXiv
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2018-agreement_based_learning" class="col p-0">
      <h5 class="title mb-0">Agreement-based Learning.</h5>
      <div class="author">
        
          
          
            
              <nobr><em>Emmanouil A Platanios</em>.</nobr>
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In arXiv (1806.01258)
          
          
            2018.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2018-agreement_based_learning-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2018-agreement_based_learning-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2018_agreement_based_learning/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2018-agreement_based_learning-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Model selection is a problem that has occupied machine learning researchers for a long time. Recently, its importance has become evident through applications in deep learning. We propose an agreement-based learning framework that prevents many of the pitfalls associated with model selection. It relies on coupling the training of multiple models by encouraging them to agree on their predictions while training. In contrast with other model selection and combination approaches used in machine learning, the proposed framework is inspired by human learning. We also propose a learning algorithm defined within this framework which manages to significantly outperform alternatives in practice, and whose performance improves further with the availability of unlabeled data. Finally, we describe a number of potential directions for developing more flexible agreement-based learning algorithms.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://arxiv.org" target="_blank">
          arXiv
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2018-deep_graphs" class="col p-0">
      <h5 class="title mb-0">Deep Graphs.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              and
              
                
                  <nobr>Alex Smola</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In arXiv (1806.01235)
          
          
            2018.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2018-deep_graphs-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2018-deep_graphs-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2018_deep_graphs/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2018-deep_graphs-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We propose an algorithm for deep learning on networks and graphs. It relies on the notion that many graph algorithms, such as PageRank, Weisfeiler-Lehman, or Message Passing can be expressed as iterative vertex updates. Unlike previous methods which rely on the ingenuity of the designer, Deep Graphs are adaptive to the estimation problem. Training and deployment are both efficient, since the cost is O(|E|+|V|), where E and V are the sets of edges and vertices respectively. In short, we learn the recurrent update functions rather than positing their specific functional form. This yields an algorithm that achieves excellent accuracy on both graph labeling and regression tasks.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2018</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="https://arxiv.org" target="_blank">
          arXiv
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2017-active_learning" class="col p-0">
      <h5 class="title mb-0">Active Learning amidst Logical Knowledge.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.microsoft.com/en-us/research/people/akapoor/" target="_blank">Ashish Kapoor</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://erichorvitz.com" target="_blank">Eric Horvitz</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In arXiv (1709.08850)
          
          
            2017.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2017-active_learning-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2017-active_learning-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2017_active_learning/paper.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2017-active_learning-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Structured prediction is ubiquitous in applications of machine learning such as knowledge extraction and natural language processing. Structure often can be formulated in terms of logical constraints. We consider the question of how to perform efficient active learning in the presence of logical constraints among variables inferred by different classifiers. We propose several methods and provide theoretical results that demonstrate the inappropriateness of employing uncertainty guided sampling, a commonly used active learning method. Furthermore, experiments on ten different datasets demonstrate that the methods significantly outperform alternatives in practice. The results are of practical significance in situations where labeled data is scarce.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.nips.cc" target="_blank">
          NeurIPS
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2017-accuracy_estimation_logic" class="col p-0">
      <h5 class="title mb-0">Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://www.microsoft.com/en-us/research/people/hoifung/" target="_blank">Hoifung Poon</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://erichorvitz.com" target="_blank">Eric Horvitz</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~tom/" target="_blank">Tom Mitchell</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Neural Information Processing Systems
          
          
            2017.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2017-accuracy_estimation_logic-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2017-accuracy_estimation_logic-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2017_logic/paper.pdf" target="_blank">PDF</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2017_logic/supplementary.pdf" target="_blank">Supplementary</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2017_logic/poster.pdf" target="_blank">Poster</a>
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="http://arxiv.org/abs/1705.07086" target="_blank">arXiv</a>
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2017-accuracy_estimation_logic-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We propose an efﬁcient method to estimate the accuracy of classiﬁers using only unlabeled data. We consider a setting with multiple classiﬁcation problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them. The proposed method is based on the intuition that: (i) when classiﬁers agree, they are more likely to be correct, and (ii) when the classiﬁers make a prediction that violates the constraints, at least one classiﬁer must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classiﬁer outputs. The results emphasize the utility of logical constraints in estimating accuracy, thus validating our intuition.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2017</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.icml.cc" target="_blank">
          ICML
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2016ti" class="col p-0">
      <h5 class="title mb-0">Estimating Accuracy from Unlabeled Data: A Bayesian Approach.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://sites.google.com/site/kumaravinavadubey/" target="_blank">Avinava Dubey</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~tom/" target="_blank">Tom Mitchell</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In International Conference in Machine Learning
          
          
            2016.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2016ti-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2016ti-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2016ti/platanios_2016ti.pdf" target="_blank">PDF</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2016ti/platanios_2016ti_supp.pdf" target="_blank">Supplementary</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2016ti/platanios_2016ti_poster.pdf" target="_blank">Poster</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2016ti/platanios_2016ti_slides.pdf" target="_blank">Slides</a>
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2016ti-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We consider the question of how unlabeled data can be used to estimate the true accuracy of learned classifiers, and the related question of how outputs from several classifiers performing the same task can be combined based on their estimated accuracies. To answer these questions, we first present a simple graphical model that performs well in practice. We then provide two nonparametric extensions to it that improve its performance. Experiments on two real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2016</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.aaai.org" target="_blank">
          AAAI
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Mitchell-2015wo" class="col p-0">
      <h5 class="title mb-0">Never-Ending Learning.</h5>
      <div class="author">
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~tom/" target="_blank">Tom Mitchell</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>William W Cohen,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>Estevam R Hruschka Jr,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://talukdar.net/" target="_blank">Partha Pratim Talukdar</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~jbetter/" target="_blank">Justin Betteridge</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~acarlson/" target="_blank">Andrew Carlson</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://allenai.org/team/bhavanad/" target="_blank">Bhanava Dalvi</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~mg1/" target="_blank">Matt Gardner</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>Bryan Kisiel,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://allenai.org/team/jayantk/" target="_blank">Jayant Krishnamurthy</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~nlao/" target="_blank">Ni Lao</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~krivard/" target="_blank">Kathryn Mazaitis</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr>Thahir P Mohamed,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://nakashole.com/" target="_blank">Ndapakula Nakashole</a>,</nobr>
                
              
            
          
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://aritter.github.io/" target="_blank">Alan Ritter</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.mehdisamadi.com/" target="_blank">Mehdi Samadi</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://burrsettles.com/" target="_blank">Burr Settles</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~rcwang/" target="_blank">Richard C Wang</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~dwijaya/" target="_blank">Derry Wijaya</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~abhinavg/" target="_blank">Abhinav Gupta</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~xinleic/" target="_blank">Xinlei Chen</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="https://asaparov.org/" target="_blank">Abulhair Saparov</a>,</nobr>
                
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.malcolmgreaves.io/" target="_blank">Malcolm Greaves</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://staff.psc.edu/welling/" target="_blank">Joel Welling</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Association for the Advancement of Artificial Intelligence
          
          
            2015.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Mitchell-2015wo-abstract" role="button" aria-expanded="false" aria-controls="Mitchell-2015wo-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/mitchell_2015wo/mitchell_2015wo.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Mitchell-2015wo-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Whereas people learn many different types of knowledge from diverse experiences over many years, most current machine learning systems acquire just a single function or data model from just a single data set. We propose a neverending learning paradigm for machine learning, to better reflect the more ambitious and encompassing type of learning performed by humans. As a case study, we describe the Never-Ending Language Learner (NELL), which achieves some of the desired properties of a never-ending learner, and we discuss lessons learned. NELL has been learning to read the web 24 hours/day since January 2010, and so far has acquired a knowledge base with over 80 million confidenceweighted beliefs (e.g., servedWith(tea, biscuits)). NELL has also learned millions of features and parameters that enable it to read these beliefs from the web. Additionally, it has learned to reason over these beliefs to infer new beliefs, and is able to extend its ontology by synthesizing new relational predicates. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.ml.cmu.edu" target="_blank">
          CMU
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2015ui" class="col p-0">
      <h5 class="title mb-0">Estimating Accuracy from Unlabeled Data.</h5>
      <div class="author">
        
          
          
            
              <nobr><em>Emmanouil Antonios Platanios</em>.</nobr>
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Master’s Thesis at Carnegie Mellon University
          
          
            2015.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2015ui-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2015ui-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2015ui/platanios_2015ui.pdf" target="_blank">PDF</a>
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2015ui/platanios_2015ui_poster.pdf" target="_blank">Poster</a>
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2015ui-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We consider the question of how unlabeled data can be used to estimate the true accuracy of learned classiﬁers. This is an important question for any autonomous learning system that must estimate its accuracy without supervision, and also when classiﬁers trained from one data distribution must be applied to a new distribution (e.g., document classiﬁers trained on one text corpus are to be applied to a second corpus). We ﬁrst show how to estimate error rates exactly from unlabeled data when given a collection of competing classiﬁers that make independent errors, based on the agreement rates between subsets of these classiﬁers. We further show that even when the competing classiﬁers do not make independent errors, both their accuracies and error dependencies can be estimated by making certain relaxed assumptions. We then present an alternative approach based on graphical models that also allows us to combine the outputs of the classiﬁers into a single output label. A simple graphical model is introduced that performs well in practice. Then, two nonparametric extensions to it are presented, that signiﬁcantly improve its performance. Experiments on two real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. We also obtain results demonstrating our graphical model approaches beating alternative methods for combining the classiﬁers’ outputs. These results are of practical signiﬁcance in situations where labeled data is scarce and shed light on the more general question of how the consistency among multiple functions is related to their true accuracies.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2015</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.auai.org" target="_blank">
          UAI
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2014ti" class="col p-0">
      <h5 class="title mb-0">Estimating Accuracy from Unlabeled Data.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~avrim/" target="_blank">Avrim Blum</a>,</nobr>
                
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="http://www.cs.cmu.edu/~tom/" target="_blank">Tom Mitchell</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Conference on Uncertainty in Artificial Intelligence
          
          
            2014.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2014ti-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2014ti-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2014ti/platanios_2014ti.pdf" target="_blank">PDF</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2014ti/platanios_2014ti_addendum.pdf" target="_blank">Addendum</a>
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2014ti/platanios_2014ti_poster.pdf" target="_blank">Poster</a>
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2014ti-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            We consider the question of how unlabeled data can be used to estimate the true accuracy of learned classifiers. This is an important question for any autonomous learning system that must estimate its accuracy without supervision, and also when classifiers trained from one data distribution must be applied to a new distribution (e.g., document classifiers trained on one text corpus are to be applied to a second corpus). We first show how to estimate error rates exactly from unlabeled data when given a collection of competing classifiers that make independent errors, based on the agreement rates between subsets of these classifiers. We further show that even when the competing classifiers do not make independent errors, both their accuracies and error dependencies can be estimated by making certain relaxed assumptions. Experiments on two data real-world data sets produce estimates within a few percent of the true accuracy, using solely unlabeled data. These results are of practical significance in situations where labeled data is scarce and shed light on the more general question of how the consistency among multiple functions is related to their true accuracies.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?reload=true&amp;punumber=34" target="_blank">
          PAMI
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2014gp" class="col p-0">
      <h5 class="title mb-0">Gaussian Process-Mixture Conditional Heteroscedasticity.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="https://www.cut.ac.cy/eecei/staff/sotirios.chatzis/" target="_blank">Sotirios Chatzis</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In IEEE Transactions on Pattern Analysis and Machine Intelligence
          
          
            2014.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2014gp-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2014gp-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2014gp/platanios_2014gp.pdf" target="_blank">PDF</a>
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2014gp-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Generalized autoregressive conditional heteroscedasticity (GARCH) models have long been considered as one of the most successful families of approaches for volatility modeling in ﬁnancial return series. In this paper, we propose an alternative approach based on methodologies widely used in the ﬁeld of statistical machine learning. Speciﬁcally, we propose a novel nonparametric Bayesian mixture of Gaussian process regression models, each component of which models the noise variance process that contaminates the observed data as a separate latent Gaussian process driven by the observed data. This way, we essentially obtain a Gaussian process-mixture conditional heteroscedasticity (GPMCH) model for volatility modeling in ﬁnancial return series. We impose a nonparametric prior with power-law nature over the distribution of the model mixture components, namely the Pitman-Yor process prior, to allow for better capturing modeled data distributions with heavy tails and skewness. Finally, we provide a copula-based approach for obtaining a predictive posterior for the covariances over the asset returns modeled by means of a postulated GPMCH model. We evaluate the efﬁcacy of our approach in a number of benchmark scenarios, and compare its performance to state-of-the-art methodologies.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2014</h3>
    </div>
  </div>

<div class="row m-0 p-0" style="border-bottom: 1px solid #ddd;">
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
  <div class="col-sm-1 p-0 abbr">
    
      
        <a class="badge font-weight-bold danger-color-dark align-middle" style="width: 53px;" href="http://www.nips.cc" target="_blank">
          NeurIPS
        </a>
      
    
  </div>
  <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">
    
    <div id="Platanios-2012uh" class="col p-0">
      <h5 class="title mb-0">Nonparametric Mixtures of Multi-Output Heteroscedastic Gaussian Processes for Volatility Modeling.</h5>
      <div class="author">
        
          
          
            
              
                <nobr><em>Emmanouil Antonios Platanios</em>,</nobr>
              
            
          
        
          
          
            
              and
              
                
                  <nobr><a href="https://www.cut.ac.cy/eecei/staff/sotirios.chatzis/" target="_blank">Sotirios Chatzis</a>.</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Neural Information Processing Systems Workshop on Modern Nonparametric Methods in Machine Learning
          
          
            2012.
          
        </p>
      </div>

      
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#Platanios-2012uh-abstract" role="button" aria-expanded="false" aria-controls="Platanios-2012uh-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2012uh/platanios_2012uh.pdf" target="_blank">PDF</a>
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="/assets/pdf/platanios_2012uh/platanios_2012uh_poster.pdf" target="_blank">Poster</a>
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="Platanios-2012uh-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            In this work, we present a nonparametric Bayesian method for multivariate volatility modeling. Our approach is based on postulation of a novel mixture of multioutput heteroscedastic Gaussian processes to model the covariance matrices of multiple assets. Speciﬁcally, we use the Pitman-Yor process prior as the non- parametric prior imposed over the components of our model, which are taken as multioutput heteroscedastic Gaussian processes obtained by introducing appropriate convolution kernels that combine simple heteroscedastic Gaussian processes under a multioutput scheme. We exhibit the efﬁcacy of our approach in a volatility prediction task.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
    <div class="col-sm-1 align-self-end mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2012</h3>
    </div>
  </div>



  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2022 Anthony Platanios.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- GitHub Stars -->
  <script src="/assets/js/github-stars.js"></script>
  <script type="text/javascript">
    
      
    
      
    
      
    
      
        
        githubStars("eaplatanios/symphony-mt", function(stars) { $("#curriculum-learningeaplatanios-symphony-mt-stars").text('' + stars); });
      
    
      
        
        githubStars("eaplatanios/jelly-bean-world", function(stars) { $("#jelly-bean-worldeaplatanios-jelly-bean-world-stars").text('' + stars); });
      
    
      
        
        githubStars("eaplatanios/symphony-mt", function(stars) { $("#machine-translationeaplatanios-symphony-mt-stars").text('' + stars); });
      
    
      
    
      
    
      
        
        githubStars("eaplatanios/tensorflow_scala", function(stars) { $("#TensorFlow-Scalaeaplatanios-tensorflow_scala-stars").text('' + stars); });
      
    
  </script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-54519238-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
